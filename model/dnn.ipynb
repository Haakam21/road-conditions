{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLModelPotholes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haakam21/road-conditions/blob/master/model/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4-jw63Labo4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3e9f188f-c675-4224-8aae-08a200487360"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rGLMLkXm2Mp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "943369c3-e8c8-40fc-fcc7-bbc11292f5f3"
      },
      "source": [
        "#import statements\n",
        "from pandas import read_excel\n",
        "from array import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import History \n",
        "from keras.callbacks import Callback,ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "history = History()\n",
        "\n",
        "#reading the data file in\n",
        "my_sheet = 'Sheet 1'\n",
        "file_name = './Data.xlsx'\n",
        "df = read_excel(file_name, sheet_name = my_sheet)\n",
        "\n",
        "#print data file/excel file\n",
        "print(df)\n",
        "\n",
        "#get dimensions of the data\n",
        "shape = df.shape\n",
        "rows = shape[0]\n",
        "coloumns = shape[1] - 2\n",
        "\n",
        "#print out the dimensions of the data that we will use\n",
        "print(rows)\n",
        "print(coloumns)\n",
        "\n",
        "\n",
        "\n",
        "#make the x_training and y_training data\n",
        "x_training = [[0 for x in range(coloumns)] for y in range(rows)]\n",
        "y_training=[[0 for x in range(1)] for y in range(rows)]\n",
        "x_trainingPandas = [[0 for x in range(coloumns)] for y in range(rows)]\n",
        "y_trainingPandas =[[0 for x in range(1)] for y in range(rows)]\n",
        "namesOfColoumns = ['acc_x', 'acc_y', 'acc_z', 'acc_dx', 'acc_dy', 'acc_dz', 'pothole', 'date', '(acc_z)^2', '(acc_dz)^2']\n",
        "\n",
        "print('rows is', len(x_training))\n",
        "print('coloumns is', len(x_training[0]))\n",
        "\n",
        "#used to compute f1 score\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code + a website\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val\n",
        "\n",
        "\n",
        "#fill up the data in x_training and y_training\n",
        "#iterate over all of the data points\n",
        "for ind in df.index: \n",
        "  for index in range(10):\n",
        "    if index != 6 and index != 7:\n",
        "      if index > 7:\n",
        "        x_training[ind][index - 2] = df[namesOfColoumns[index]][ind]\n",
        "        #print(x_training[ind][index - 2]) #print statements to see it run\n",
        "      elif index < 6:\n",
        "        x_training[ind][index] = df[namesOfColoumns[index]][ind]\n",
        "        #print(x_training[ind][index]) #print statements to see it run\n",
        "\n",
        "\n",
        "for ind in df.index: \n",
        "  y_training[ind][0] = df[namesOfColoumns[6]][ind]\n",
        "  #print(y_training[ind][0]). #print statement to see the y values\n",
        "\n",
        "\n",
        "\n",
        "#now that the data is loaded correctly, time to create and run the model!\n",
        "# define the DL model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "x_trainingPandas = x_training\n",
        "y_trainingPandas = y_training\n",
        "x_training=np.array(x_training)\n",
        "y_training=np.array(y_training)\n",
        "\n",
        "# compile the DL model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[get_f1, 'accuracy', tf.keras.metrics.AUC()]) #adam optimizer, binary crossentropy (classification) for loss function\n",
        "\n",
        "# fit the keras model on the dataset\n",
        "model.fit(x_training, y_training, epochs=300, batch_size=512, callbacks=[history])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4iaMgJEcJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c30acc9-dd20-4b7a-c292-8f1efbd76294"
      },
      "source": [
        "# # see the final accuracy of the model\n",
        "# _, accuracy = model.evaluate(x_training, y_training)\n",
        "# print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "\n",
        "# should write out the history of the model's performance\n",
        "print(history.history.keys())\n",
        "wordKeys = list(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history[wordKeys[2]])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history[wordKeys[0]])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "#history for f1_score\n",
        "plt.plot(history.history[wordKeys[1]])\n",
        "plt.title('f1_score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#history for auc\n",
        "plt.plot(history.history[wordKeys[3]])\n",
        "# plt.plot([0, 1, 2, 3])\n",
        "plt.title('auroc')\n",
        "plt.ylabel('auroc score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKBg8bw5Bcpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "1b7e6ef4-2990-4d58-979d-d76cf85ad7c6"
      },
      "source": [
        "print(len(y_training))\n",
        "\n",
        "countPotholePos = 0\n",
        "countPotholeNeg = 0\n",
        "for indexForPotCount in range(7100):\n",
        "  if y_training[indexForPotCount][0] < 0.5:\n",
        "    countPotholeNeg = countPotholeNeg + 1\n",
        "  else:\n",
        "    countPotholePos = countPotholePos + 1\n",
        "print('number of negative potholes is', countPotholeNeg)\n",
        "print('number of positive potholes is', countPotholePos)\n",
        "print('Percentage of potholes data that are have a pothole', (countPotholePos)/(countPotholeNeg + countPotholePos) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKXlCx1pTzTb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "88bbb1fa-a794-4316-a8ae-d58a719ac501"
      },
      "source": [
        "for layer in model.layers:\n",
        "    weights = layer.get_weights()\n",
        "    print(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r88ZSS4WYZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7230327a-0c22-41c3-db00-f581dc824d3d"
      },
      "source": [
        "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from prettytable import ALL\n",
        "from sklearn.metrics import f1_score\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)\n",
        "\n",
        "def f1_loss(y_true, y_pred):\n",
        "    \n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return 1 - K.mean(f1)\n",
        "\n",
        "\n",
        "# define the NEW DL model\n",
        "modelF = Sequential()\n",
        "modelF.add(Dense(12, input_dim=8, activation='relu'))\n",
        "modelF.add(Dropout(0.4))\n",
        "modelF.add(Dense(12, activation='relu'))\n",
        "modelF.add(Dropout(0.4))\n",
        "modelF.add(Dense(8, activation='relu'))\n",
        "modelF.add(Dropout(0.4))\n",
        "modelF.add(Dense(1, activation='sigmoid'))\n",
        "x_training=np.array(x_training)\n",
        "y_training=np.array(y_training)\n",
        "\n",
        "# compile the DL model\n",
        "modelF.compile(loss=f1_loss, optimizer='adam', metrics=[f1, 'accuracy', tf.keras.metrics.AUC()]) #adam optimizer, binary crossentropy (classification) for loss function\n",
        "\n",
        "# fit the keras model on the dataset\n",
        "modelF.fit(x_training, y_training, epochs=1000, batch_size=1024, callbacks=[history])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXdJpm-u1Dq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672a86ee-b9f6-45e2-abd0-87d882d88493"
      },
      "source": [
        "# # see the final accuracy of the model\n",
        "# _, accuracy = model.evaluate(x_training, y_training)\n",
        "# print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "# should write out the history of the model's performance\n",
        "print(history.history.keys())\n",
        "wordKeys = list(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history[wordKeys[2]])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history[wordKeys[0]])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "#history for f1_score\n",
        "plt.plot(history.history[wordKeys[1]])\n",
        "plt.title('f1_score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#history for auc\n",
        "plt.plot(history.history[wordKeys[3]])\n",
        "# plt.plot([0, 1, 2, 3])\n",
        "plt.title('auroc')\n",
        "plt.ylabel('auroc score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VAjW-7E7cQS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b05e0b90-97a5-4f36-95ce-ca6bb2fb4f6f"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "#NOW TRYING THIS WITH OVERSAMPLING THE DATA\n",
        "sm = SMOTE(random_state=12, ratio = 1.0)\n",
        "x_res, y_res = sm.fit_sample(x_trainingPandas, y_trainingPandas)\n",
        "\n",
        "print(len(y_res))\n",
        "print(len(x_res))\n",
        "\n",
        "countPotholePos = 0\n",
        "countPotholeNeg = 0\n",
        "for indexForPotCount in range(len(y_res)):\n",
        "  if y_res[indexForPotCount] < 0.5:\n",
        "    countPotholeNeg = countPotholeNeg + 1\n",
        "  else:\n",
        "    countPotholePos = countPotholePos + 1\n",
        "print('number of negative potholes is', countPotholeNeg)\n",
        "print('number of positive potholes is', countPotholePos)\n",
        "print('Percentage of potholes data that are have a pothole', (countPotholePos)/(countPotholeNeg + countPotholePos) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3-KdCYp9yhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1ebeb30-3832-4bd1-fd73-669a403b1220"
      },
      "source": [
        "# define the NEW DL model\n",
        "modelSample = Sequential()\n",
        "modelSample.add(Dense(12, input_dim=8, activation='relu'))\n",
        "modelSample.add(Dropout(0.4))\n",
        "modelSample.add(Dense(12, activation='relu'))\n",
        "modelSample.add(Dropout(0.4))\n",
        "modelSample.add(Dense(8, activation='relu'))\n",
        "modelSample.add(Dropout(0.4))\n",
        "modelSample.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the DL model\n",
        "modelSample.compile(loss=f1_loss, optimizer='adam', metrics=[f1, 'accuracy', tf.keras.metrics.AUC()]) #adam optimizer, binary crossentropy (classification) for loss function\n",
        "\n",
        "# fit the keras model on the dataset\n",
        "modelSample.fit(x_res, y_res, epochs=1000, batch_size=1024, callbacks=[history])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRPHAozR-3mx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1370606-45d2-413c-d79b-56f005ea13d2"
      },
      "source": [
        "# should write out the history of the model's performance\n",
        "print(history.history.keys())\n",
        "wordKeys = list(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history[wordKeys[2]])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history[wordKeys[0]])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "#history for f1_score\n",
        "plt.plot(history.history[wordKeys[1]])\n",
        "plt.title('f1_score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#history for auc\n",
        "plt.plot(history.history[wordKeys[3]])\n",
        "# plt.plot([0, 1, 2, 3])\n",
        "plt.title('auroc')\n",
        "plt.ylabel('auroc score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDz7RAAF_TxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fed01006-3bfc-436c-85bf-b0a5b0f9a63e"
      },
      "source": [
        "# define the NEW DL model\n",
        "modelAccuracy = Sequential()\n",
        "modelAccuracy.add(Dense(12, input_dim=8, activation='relu'))\n",
        "modelAccuracy.add(Dropout(0.4))\n",
        "modelAccuracy.add(Dense(12, activation='relu'))\n",
        "modelAccuracy.add(Dropout(0.4))\n",
        "modelAccuracy.add(Dense(12, activation='relu'))\n",
        "modelAccuracy.add(Dropout(0.4))\n",
        "modelAccuracy.add(Dense(8, activation='relu'))\n",
        "modelAccuracy.add(Dropout(0.4))\n",
        "modelAccuracy.add(Dense(4, activation='relu'))\n",
        "modelAccuracy.add(Dropout(0.4))\n",
        "modelAccuracy.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile the DL model\n",
        "modelAccuracy.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1, 'accuracy', tf.keras.metrics.AUC()]) #adam optimizer, binary crossentropy (classification) for loss function\n",
        "\n",
        "# fit the keras model on the dataset\n",
        "modelAccuracy.fit(x_res, y_res, epochs=400, batch_size=128, callbacks=[history])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIc2k-UM_5Ht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9583e42a-1d85-43f6-a4cb-45e9cc8cad7c"
      },
      "source": [
        "# should write out the history of the model's performance\n",
        "print(history.history.keys())\n",
        "wordKeys = list(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history[wordKeys[2]])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history[wordKeys[0]])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "#history for f1_score\n",
        "plt.plot(history.history[wordKeys[1]])\n",
        "plt.title('f1_score')\n",
        "plt.ylabel('f1 score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#history for auc\n",
        "plt.plot(history.history[wordKeys[3]])\n",
        "# plt.plot([0, 1, 2, 3])\n",
        "plt.title('auroc')\n",
        "plt.ylabel('auroc score')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVca52tjKtkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TO-DO (note for myself):\n",
        "# 1. Split up data for training and validation\n",
        "# 2. Compute F1 Scores\n",
        "# 3. Research about how to deal with skewed data more effectively"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}